


























































ar
X

iv
:0

70
4.

35
04

v1
  [

qu
an

t-
ph

] 
 2

6 
A

pr
 2

00
7

Smooth Rényi Entropy of Ergodic Quantum
Information Sources

Berry Schoenmakers Jilles Tjoelker
Dept. of Mathematics and Computer Science

Technical University Eindhoven
The Netherlands

berry@win.tue.nl j.tjoelker@student.tue.nl

Pim Tuyls
Information Security Systems
Philips Research Eindhoven

The Netherlands
pim.tuyls@philips.com

Evgeny Verbitskiy
Digital Signal Processing

Philips Research Eindhoven
The Netherlands

evgeny.verbitskiy@philips.com

Abstract— We investigate the recently introduced notion of
smooth Rényi entropy for the case of ergodic information sources,
thereby generalizing previous work which concentrated mainly
on i.i.d. information sources. We will actually consider ergodic
quantum information sources, of which ergodic classical infor-
mation sources are a special case. We prove that the average
smooth Rényi entropy rate will approach the entropy rate of a
stationary, ergodic source, which is equal to the Shannon entropy
rate for a classical source and the von Neumann entropy rate
for a quantum source.

I. INTRODUCTION

The elegant notion of smooth Rényi entropy was introduced
recently by Renner and Wolf in [6] for classical information
sources, and the natural extension to quantum information
sources was defined by Renner and König in [5]. In these two
papers and further work by Renner and Wolf [7], [4], many
properties of smooth Rényi entropy—and smooth min-entropy
and smooth max-entropy in particular—have been studied in
detail.

A central property of smooth Rényi entropy proved in these
works is that for memoryless (i.i.d.) information sources, the
average smooth Rényi entropy rate will approach the entropy
rate of the source, which is equal to the Shannon entropy for a
classical source and the von Neumann entropy for a quantum
source. Whereas, in general, the average (conventional) Rényi
entropy rate of a memoryless source does not converge to the
source’s entropy rate.

In this paper we extend the study of smooth Rényi entropy
to the more general class of stationary, ergodic sources rather
than memoryless sources. We will prove that for both the
classical and the quantum case that the average smooth Rényi
entropy rate will approach the Shannon and the von Neumann
entropy rate, respectively. We will do so by first treating the
classical case and then reducing the quantum case to the
classical one without losing generality.

In general, smooth Rényi entropy of order α > 1, and α =
∞ (min-entropy) in particular, is of cryptographic relevance
(e.g., for randomness-extraction), and smooth Rényi entropy
of order α < 1, and α = 0 (max-entropy) in particular, are
relevant to data compression (minimum encoding length). In
these contexts, the importance of smooth Rényi entropy is that
its rate is basically equal to the Shannon/von Neumann entropy

rate for an i.i.d. source (and for ergodic sources as well, as we
show in this paper). This is not the case for conventional Rényi
entropy. More generally, as shown in the papers by Renner et
al. mentioned above, smooth Rényi entropy behaves much as
Shannon/von Neumann entropy does.

In this paper we focus on the unconditional case, whereas
much of the abovementioned work by Renner et al. treats the
more general conditional case. We leave the extension to the
conditional case for future work. However, we do consider
two notions of ǫ-closeness, one based on trace distance (also
known as variational or statistical distance) and one based on
non-normalized density matrices (or probability distributions),
where the latter is more suitable to handle the conditional
case.1 Thus, we believe that our results can be extended to the
conditional case as well.

We also note that Renner [4] presents a different kind of
generalization of i.i.d. quantum sources, namely by analyzing
the smooth min-entropy of symmetric (permutation-invariant)
quantum states. Or, more precisely, states in a symmetric
subspace of H⊗n are considered, for n ∈ N. See [4, Chapter 4]
for details, which also covers the conditional case.

II. PRELIMINARIES

Throughout this paper we use P and Q to denote probability
distributions with over the same finite or countably infinite
range Z. Similarly, we use ρ and σ to denote density matrices
on the same Hilbert space of a finite or countably infinite di-
mension. These probability distributions and density matrices
are not necessarily normalized (e.g.,

∑

z P(z) < 1 if P is
non-normalized and tr(ρ) < 1 if ρ is non-normalized).

For ease of comparison we state all the preliminaries
explicitly for the classical case as well as for the quantum
case.

Definition 1 (Classical Rényi entropy): The Rényi entropy
of order α ∈ [0, ∞] of probability distribution P is

Hα(P) =
1

1 − α
log

∑

z∈Z

P(z)α,

1The trace distance was originally used in [6], [5]. The use of non-
normalized probability distributions was also shown in the full version of
[6] and used in [7]. In this paper, we extend this to the use of non-normalized
density matrices in the quantum case.

http://arxiv.org/abs/0704.3504v1


for 0 < α < ∞, α 6= 1, and Hα(P) = limβ→α Hβ(P)
otherwise.
Hence, H0(P) = log |{z ∈ Z : P(z) > 0}|, H1(P) = H(P)
(Shannon entropy) and H∞(P) = − log maxz∈Z P(z).

For a random variable Z we use Hα(Z) as a shorthand for
Hα(PZ), where PZ is the probability distribution of Z.

Smooth Rényi entropy was introduced in [6] for the classical
case. For ǫ ≥ 0, let Bǫ(P) denote either the set of probability
distributions which are ǫ-close to P, Bǫ(P) = {Q : δ(P, Q) ≤
ǫ}, or the set of non-normalized probability distributions which
are ǫ-close to P, Bǫ(P) = {Q :

∑

z∈Z Q(z) ≥ 1 − ǫ, ∀z∈Z0 ≤
Q(z) ≤ P(z)}. The first notion of ǫ-closeness, based on the
statistical distance δ(P, Q) = 1

2

∑

z∈Z |P(z)−Q(z)|, was used
in [6]. The second notion was mentioned in the full version
of [6], and used in [7].

Definition 2 (Classical smooth Rényi entropy, [6]): The ǫ-
smooth Rényi entropy of order α ∈ [0, 1) ∪ (1, ∞] of a
probability distribution P is

Hǫα(P) =

{

infQ∈Bǫ(P) Hα(Q), 0 ≤ α < 1,
supQ∈Bǫ(P) Hα(Q), 1 < α ≤ ∞.

At the end of this paper, we point out that Hǫα(P) will
actually vary, depending on which notion of ǫ-closeness is
used, leading to a maximum difference of α

α−1
log(1 − ǫ).

For a probability distribution P on, e.g., Z = {0, 1}N, we
define Pn as the probability distribution corresponding to the
restriction of the “infinite volume” distribution P to the finite
volume {0, . . . , n − 1}.

Definition 3 (Entropy rate of a classical source): For a
stationary source given by its probability measure P, we
define

h(P) = lim
n→∞

1

n
H(Pn),

hǫα(P) = lim
n→∞

1

n
Hǫα(P

n).

We will actually prove that hǫα(P) = h(P) as ǫ → 0.
We use the standard notion of typical sequences and typ-

ical sets, which are defined for any information source (not
necessarily i.i.d.). See, for instance, [2] or [3].

Definition 4 (Typical sequences, typical set): A sequence
zn ∈ {0, 1}n, n ∈ N, is called ǫ-typical if

e−n(h(P)+ǫ) ≤ P(zn) ≤ e−n(h(P)−ǫ).

The typical set T
(n)
ǫ is the set of all ǫ-typical sequences from

{0, 1}n.
In this paper we need the following consequence of the AEP,

where we refer to [2, Section 16.8] for the AEP for ergodic
sources (known as the Shannon-McMillan-Breiman theorem).

Theorem 1 (Classical AEP bounds): Let P be a stationary,
ergodic probability distribution on Z = {0, 1}N. Let ǫ > 0.
Then, for sufficiently large n,

P(T (n)ǫ ) ≥ 1 − ǫ,

and
|T (n)ǫ | ≤ e

n(h(P)+ǫ).

Definition 5 (Quantum Rényi entropy): The Rényi entropy
of order α ∈ [0, ∞] of a density matrix ρ is

Sα(ρ) =
1

1 − α
log tr(ρα)

for 0 < α < ∞, α 6= 1, and Sα(ρ) = limβ→α Sβ(ρ)
otherwise.
Hence, S0(ρ) = log rank(ρ), S1(ρ) = S(ρ) = − tr(ρ log ρ)
(von Neumann entropy) and S∞(ρ) = − log λmax(ρ).

Analogous to the classical case, smooth Rényi entropy is
defined in the quantum case (see [5]). We use either the set
of density matrices which are ǫ-close to ρ, Bǫ(ρ) = {σ :
δ(ρ, σ) ≤ ǫ} or the set of non-normalized density matrices
which are ǫ-close to ρ, Bǫ(ρ) = {σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤
ρ}. The first notion of ǫ-closeness, based on the trace distance
δ(ρ, σ) = 1

2
tr(|ρ − σ|), was used in [5]. The second notion is

introduced here, and will actually be used in the next section.

Definition 6 (Quantum smooth Rényi entropy, [5]): The ǫ-
smooth Rényi entropy of order α ∈ [0, 1)∪(1, ∞] of a density
matrix ρ is

Sǫα(ρ) =

{

infσ∈Bǫ(ρ) Sα(σ), 0 ≤ α < 1,
supσ∈Bǫ(ρ) Sα(σ), 1 < α ≤ ∞.

Definition 7 (Entropy rates of a quantum source): For a
stationary quantum source ρ, given by its local densities
ρ(n) = ρ0,...,n−1, for n ∈ N, we define:

s(ρ) = lim
n→∞

1

n
S(ρ(n)),

sǫα(ρ) = lim
n→∞

1

n
Sǫα(ρ

(n)).

We use the following notion of typical states and typical
subspaces, as can be found in [1] (also see [3]).

Definition 8 (Typical state, typical subspace): A pure state
|e

(n)
i 〉, where e

(n)
i is an eigenvector of ρ

(n) is called ǫ-typical
if the corresponding eigenvalue λ

(n)
i satisfies

e−n(s(ρ)+ǫ) ≤ λ
(n)
i ≤ e

−n(s(ρ)−ǫ).

The typical subspace T
(n)
ǫ is the subspace spanned by all ǫ-

typical states.
We will need the following consequences of the quantum

AEP for ergodic sources, which has been studied in [1] (see
[3] for the quantum AEP for i.i.d. sources).

Theorem 2 (Quantum AEP bounds): Let ρ be a stationary,
ergodic quantum source with local densities ρ(n). Let ǫ > 0.
Then, for sufficiently large n,

tr(ρ(n)P
T

(n)
ǫ

) ≥ 1 − ǫ,

where P
T

(n)
ǫ

is the projector onto the subspace T
(n)
ǫ . Further-

more,
tr(P

T
(n)
ǫ

) ≤ en(s(ρ)+ǫ).
Clearly, the quantum AEP for ergodic sources implies the
classical AEP for ergodic sources.

The following theorem by Renner and Wolf states that
smooth Rényi entropy approaches Shannon entropy in the case
of a classical i.i.d. source.



Theorem 3 ([7, Lemma I.2]): Let Zn denote an n-tuple of
i.i.d. random variables with probability distribution PZ . Then,

lim
ǫ→0

lim
n→∞

1

n
Hǫα(Z

n) = H(Z),

for any α ∈ [0, ∞].
The analogous theorem by Renner and König for a quantum

i.i.d. source is as follows.
Theorem 4 ([5, Lemma 3]): Let ρ be a density matrix.

Then,

lim
ǫ→0

lim
n→∞

1

n
Sǫα(ρ

⊗n) = S(ρ),

for any α ∈ [0, ∞].

III. MAIN RESULT

We extend the results by Renner and Wolf (Theorem 3
above) and by Renner and König (Theorem 4 above) to the
case of ergodic sources. Throughout this section, we use
the notion of ǫ-closeness based on non-normalized proba-
bility distributions and density matrices, so Bǫ(P) = {Q :
∑

z∈Z Q(z) ≥ 1 − ǫ, ∀z∈Z0 ≤ Q(z) ≤ P(z)} and B
ǫ(ρ) =

{σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤ ρ}, respectively. In the next
section, we will argue that the results are independent on which
notion of ǫ-closeness is used.

A. Classical Case

We start with our main result for the classical case. The
known result for an i.i.d. source is by Renner and Wolf,
Theorem 3 above. We will extend this to a stationary, ergodic
source in Theorem 5 below.

Lemma 1: Let P be a stationary, ergodic information source
given by its probability measure and let 0 < ǫ < 1/2. Then
we have,

h(P) − ǫ ≤ hǫ∞(P) ≤ h(P) + 2ǫ.
Proof: Let 0 < ǫ < 1/2. To prove the lower bound, we

show that, for sufficiently large n, Hǫ∞(P
n) ≥ n(h(P) − ǫ).

Define non-normalized probability distribution Q for all zn ∈
{0, 1}n by

Q(zn) =

{

P(zn), if zn ∈ T
(n)
ǫ

0, if zn /∈ T
(n)
ǫ .

(1)

Clearly, 0 ≤ Q(zn) ≤ P(zn) and, by the AEP, Q(T
(n)
ǫ ) =

P(T
(n)
ǫ ) ≥ 1 − ǫ for sufficiently large n. So, Q ∈

Bǫ(Pn). Furthermore, for zn ∈ T
(n)
ǫ , we have that

− log P(zn) ≥ n(h(P) − ǫ), and hence that for any zn that
− log Q(zn) ≥ n(h(P) − ǫ). This implies that H∞(Q) =
− log maxzn Q(z

n) ≥ n(h(P) − ǫ) and the lower bound
follows.

Next, to prove the upper bound, we show that, for suffi-
ciently large n, one has that for all Q ∈ Bǫ(Pn),

H∞(Q) = − log max
zn

Q(zn) ≤ n(h(P) + 2ǫ).

This follows from max
zn∈T

(n)
ǫ

Q(zn) ≥ e−n(h(P)+2ǫ), which

in turn follows from
∑

zn∈T
(n)
ǫ

Q(zn) ≥ |T
(n)
ǫ |e

−n(h(P)+2ǫ).

From the AEP we get |T
(n)
ǫ | ≤ e

n(h(P)+ǫ), hence it suffices
to prove that, for sufficiently large n,

∑

zn∈T
(n)
ǫ

Q(zn) ≥ e−nǫ. (2)

As
∑

zn Q(z
n) ≥ 1 − ǫ, for Q ∈ Bǫ(Pn), and also

∑

zn /∈T
(n)
ǫ

Q(zn) ≤ ǫ (because Q(zn) ≤ P(zn) and

P(T
(n)
ǫ ) ≥ 1 − ǫ from the AEP), we only need to observe

that
1 − 2ǫ > e−nǫ

holds for sufficiently large n, using that ǫ < 1/2.
We now state an analogous lemma for the max-entropy.
Lemma 2: Let P be a stationary, ergodic information source

given by its probability measure and let 0 < ǫ < 1/2. Then
we have,

h(P) − 2ǫ ≤ hǫ0(P) ≤ h(P) + ǫ.
Proof: Let 0 < ǫ < 1/2. To prove the upper bound, we

show that, for sufficiently large n, Hǫ0(P
n) ≤ n(h(P)+ǫ). We

do so by showing that H0(Q) = log |{z
n : Q(zn) > 0}| ≤

n(h(P)+ǫ) for the non-normalized probability distribution Q,
defined by (1) in the proof of Lemma 1. As

|{zn : Q(zn) > 0}| = |{zn ∈ T (n)ǫ : Q(z
n) > 0}| ≤ |T (n)ǫ |,

the result follows directly from the AEP.
Next, to prove the lower bound, we show that, for suffi-

ciently large n, one has that for all Q ∈ Bǫ(Pn),

H0(Q) = log |{z
n : Q(zn) > 0}| ≥ n(h(P) − 2ǫ).

This is implied by |{zn ∈ T
(n)
ǫ : Q(z

n) > 0}| ≥ en(h(P)−2ǫ),
which is in turn implied by

∑

zn∈T
(n)
ǫ

Q(zn) ≥ max
zn∈T

(n)
ǫ

,Q(zn)>0

Q(zn)en(h(P)−2ǫ).

Using inequality (2) from the proof of Lemma 1, it suffices
to show that

max
zn∈T

(n)
ǫ

,Q(zn)>0

Q(zn) ≤ e−nǫe−n(h(P)−2ǫ) = e−n(h(P)−ǫ).

This is a direct consequence of the definition of ǫ-typical
sequences, as Q(zn) ≤ P(zn) for Q ∈ Bǫ(Pn), using that
Q(zn) > 0 holds for at least one zn ∈ T

(n)
ǫ on account of

inequality (2).
Theorem 5: For α ∈ [0, ∞], the ǫ-smooth entropy of a

stationary, ergodic information source P given by its proba-
bility measure on Z = {0, 1}N is close to the mean Shannon
entropy:

lim
ǫ→0

hǫα(P) = h(P).

Proof: For α < 1, the monotonicity of smooth Rényi
entropy (see, e.g., [7, Lemma 1]) yields Hǫα(P

n) ≤ Hǫ0(P
n),

and hence hǫα(P) ≤ h(P) + ǫ by Lemma 2.
To get a lower bound for hǫα(P), we note that

Hǫα(P
n) ≥ H2ǫ0 (P

n) −
log(1/ǫ)

1 − α
,



using [7, Lemma 2]. So, hǫα(P) ≥ h
2ǫ
0 (P) as the constant term

on the right-hand side vanishes for n → ∞. Using Lemma 2,
we thus get hǫα(P) ≥ h(P) − 4ǫ.

This proves that limǫ→0 h
ǫ
α(P) = h(P). The proof for α > 1

is completely symmetrical, hence omitted.
Note that the term 2ǫ in the upper and lower bounds of

Lemmas 1 and 2, respectively, can be improved to (1 + δ)ǫ
for any constant δ > 0. Similarly, the term 4ǫ in the proof of
Theorem 5 for the lower bound for hǫα can be improved to
(1 + δ)ǫ for any constant δ > 0.

B. Quantum Case

Although it is possible to prove the quantum case directly,
along the same lines as in the classical case, we treat the
quantum case indirectly, by reducing it to the classical case.
This leads to a more compact proof. To this end, we will
first prove Lemma 3 below, which captures the correspon-
dence between Bǫ(ρ(n)) and Bǫ(λ(n)). We only consider the
case of ǫ-closeness for non-normalized density matrices and
probability distributions (but the lemma also holds for the case
of ǫ-closeness based on trace distance).

To prove our lemma, we need Weyl’s monotonicity principle
which we recall first.

Theorem 6 (Weyl monotonicity): If A, B are m by m Her-
mitian matrices and B is positive, then λi(A) ≤ λi(A + B)
for all i = 1, . . . , m, where λi(M) is the i-th eigenvalue of
M (ordered from largest to smallest).

Lemma 3: Let ρ be a density matrix with eigenvalues λ1 ≥
λ2 ≥ . . . ≥ λm.

1) For any density matrix σ with eigenvalues µ1 ≥ µ2 ≥
. . . ≥ µm,

σ ∈ Bǫ(ρ) ⇒ µ ∈ Bǫ(λ).

2) Given real numbers µ1, . . . , µm such that µ ∈ B
ǫ(λ),

there exists a matrix σ with eigenvalues µ1, . . . , µm such
that σ ∈ Bǫ(ρ).

Proof: We prove the result for

Bǫ(λ) = {µ :
∑

i

µi ≥ 1 − ǫ, ∀i0 ≤ µi ≤ λi},

Bǫ(ρ) = {σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤ ρ}.

For the first part, let σ be a (possibly non-normalized) density
matrix with eigenvalues µ1 ≥ µ2 ≥ . . . ≥ µm and suppose
σ ∈ Bǫ(ρ). Since σ is positive we have µi ≥ 0 for all i.
And since σ ≤ ρ, we have that ρ − σ is positive as well,
so λi ≥ µi for all i (using Weyl’s monotonicity principle,
Theorem 6 above). Finally, note that tr(σ) ≥ 1−ǫ is equivalent
to

∑

i µi ≥ 1 − ǫ, so we conclude that µ ∈ B
ǫ(λ).

For the second part, let µ ∈ Bǫ(λ) be given. We write the
Hermitian matrix ρ in diagonal form,

ρ =
∑

i

λi|vi〉〈vi|.

for eigenvectors vi (i = 1, . . . , m), and we show that the
Hermitian matrix σ, defined by

σ =
∑

i

µi|vi〉〈vi|,

is in Bǫ(ρ).
Since µ ∈ Bǫ(λ), we have that 0 ≤ µi ≤ λi, and because ρ

and σ commute (eigenvalues of ρ − σ are λi − µi), we have
0 ≤ σ ≤ ρ. Clearly,

∑

i µi ≥ 1 − ǫ so tr(σ) ≥ 1 − ǫ as well,
and therefore σ ∈ Bǫ(ρ).

We now proceed to prove the main result for the quantum
case.

Theorem 7: For α ∈ [0, ∞], the ǫ-smooth entropy of
a stationary, ergodic quantum source ρ given by its local
densities ρ(n), for n ∈ N, is close to the mean von Neumann
entropy:

lim
ǫ→0

sǫα(ρ) = s(ρ).

Proof: We will apply Theorem 5 as follows.
First note that for the local densities ρ(n) for a quantum

information source ρ, we have that S(ρ(n)) = H(λ(n)), where
λ(n) denotes the probability distribution corresponding to the
eigenvalues of ρ(n). Consequently, s(ρ) = h(λ) as well, where
λ denotes the probability distribution corresponding to the
eigenvalues of ρ.

Next, we recall the definitions of smooth Rényi entropy in
the classical and quantum case, resp.:

Hǫα(P) =

{

infQ∈Bǫ(P) Hα(Q), 0 ≤ α < 1,
supQ∈Bǫ(P) Hα(Q), 1 < α ≤ ∞.

Sǫα(ρ) =

{

infσ∈Bǫ(ρ) Sα(σ), 0 ≤ α < 1,
supσ∈Bǫ(ρ) Sα(σ), 1 < α ≤ ∞.

We only consider the case α < 1, as the other case follows
by symmetry. We have that

Sǫα(ρ
(n)) = inf

σ∈Bǫ(ρ(n))
Sα(σ)

= inf
µ∈Bǫ(λ(n))

Hα(µ)

= Hǫα(λ
(n)),

using that Lemma 3 implies that the infimum over Bǫ(ρ(n))
is equal to the infimum over Bǫ(λ(n)).

As a consequence, we have that sǫα(ρ) = h
ǫ
α(λ) and the

result follows from Theorem 5. Here, we use the fact that
quantum AEP implies classical AEP.

We note that the actual convergence rate (as a function
of ǫ) is the same as in the classical case, which follows by
considering the analogons of Lemmas 1 and 2.

IV. NOTIONS OF ǫ-CLOSENESS

As mentioned in the introduction, two notions of ǫ-closeness
were originally introduced by Renner and Wolf [6], [7], which
can both be used in the definition of classical smooth Rényi
entropy. For the quantum case, the paper by Renner and König
[5] only considers the notion of ǫ-closeness based on the trace
distance. As the natural quantum analogon of the notion of
ǫ-closeness based on non-normalized probability distributions,
we have used the set of non-normalized density matrices which
are ǫ-close to a given density matrix ρ:

Bǫ(ρ) = {σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤ ρ}.



The entropy rates (Definitions 3 and 7), and consequently
the results for these entropy rates (Theorems 3, 4, 5, and 7) do
not depend on which of these notions of ǫ-closeness is used.

Furthermore, if the corresponding notions of ǫ-closeness are
used, the quantum case and the classical case are in general
connected as follows:

Sǫα(ρ) = inf
σ∈Bǫ(ρ)

Sα(σ) = inf
µ∈Bǫ(λ)

Hα(µ) = H
ǫ
α(λ),

where λ denotes the probability distribution corresponding to
the eigenvalues of ρ.

We note, however, that the smooth Rényi entropy Hǫα may
depend on which notion of ǫ-closeness is used, contrary to
what was stated before (see, e.g., Section 3.3 of the full version
of [6]). In general, one can show that

0 ≤ inf
δ(P,Q)≤ǫ

Hα(Q)− inf
P

z
Q(z)≥1−ǫ

∀z0≤Q(z)≤P(z)

Hα(Q) ≤
α

α − 1
log(1−ǫ),

for 0 ≤ α < 1, and that
α

α − 1
log(1−ǫ) ≤ sup

δ(P,Q)≤ǫ

Hα(Q)− sup
P

z
Q(z)≥1−ǫ

∀z0≤Q(z)≤P(z)

Hα(Q) ≤ 0,

for 1 < α ≤ ∞. So, only for α = 0 either notion of ǫ-
closeness yields the same value for the smooth Rényi entropy
Hǫα. But for all other values of α, the difference may be as
large as α

α−1
log(1 − ǫ). The maximum difference is attained

for the uniform distribution P(z) = 1/m on a finite range Z
of size m, assuming that ǫ is sufficiently small (i.e., ǫ < 1/m).

ACKNOWLEDGMENT

Boris Škorić is gratefully acknowledged for discussions in
the early stage of this work.

REFERENCES

[1] I. Bjelakovic and A. Szkola, ”The Data Compression Theorem
for Ergodic Quantum Information Sources”, 2003. Available as
quant-ph/0301043.

[2] T. M. Cover and J.A. Thomas, ”Elements of Information Theory”, 2nd
edition, Wiley-Interscience, 2006.

[3] M. A. Nielsen and I. L. Chuang, ”Quantum Computation and Quantum
Information”, Cambridge University Press, 2000.

[4] R. Renner, ”Security of Quantum Key Distribution”, Diss. ETH
No. 16242, PhD Thesis, ETH Zürich, September 2005. Also available
as quant-ph/0512258.

[5] R. Renner and R. König, LNCS 3378, TCC 2005, pp. 407-425.
[6] R. Renner and S. Wolf, ”Smooth Rényi Entropy and

Applications, ISIT 2004, p. 233. Full version available as
http://qi.ethz.ch/pub/publications/smooth.ps .

[7] R. Renner and S. Wolf, ”Simple and Tight Bounds for Information
Reconciliation and Privacy Amplification”, LNCS 3788, Asiacrypt 2005,
pp. 199–216.

http://arxiv.org/abs/quant-ph/0301043
http://arxiv.org/abs/quant-ph/0512258
http://qi.ethz.ch/pub/publications/smooth.ps

	Introduction
	Preliminaries
	Main Result
	Classical Case
	Quantum Case

	Notions of -Closeness
	References

