{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tp-airlines.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQLfs9cgC6l-"
      },
      "source": [
        "# ejemplo tomado de: \n",
        "# https://community.hortonworks.com/articles/84781/spark-text-analytics-uncovering-data-driven-topics.html\n",
        "# github: https://github.com/zaratsian/Spark/blob/master/text_analytics_datadriven_topics.json (con zeppelin)\n",
        "# otros ejemplos muy buenos: https://github.com/zaratsian/Spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUVs2VCYBkVe",
        "outputId": "e1dca687-f3b1-4b95-9968-a75c14b7aaa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#configuraci√≥n en google colab de spark y pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjwlzuuI7F7M",
        "outputId": "111ff598-e648-46ee-c0e2-22e1debbf4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjW7J3n9C6mH"
      },
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local', \"app-topic-detection\") \n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNHKHUNSC6mD"
      },
      "source": [
        "#from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import monotonically_increasing_id, col, expr, when, concat, lit, isnan\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Cz07ZeJ1Fx"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
        "from pyspark.sql.types import StringType,DoubleType,IntegerType,ArrayType\n",
        "#from pyspark.sql.functions import *\n",
        "from pyspark.ml.linalg import Vectors, SparseVector\n",
        "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHO9xo0_JPpt"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFHq6rlnJmed"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKxSznZPJ8iq"
      },
      "source": [
        "# stopwords en nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words_nltk = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMZSzEWVC6mK",
        "outputId": "b8cb439e-7062-4826-dd4f-6c61a57e8fd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df=spark.read.csv(\"gdrive/My Drive/datasets/airlines.csv\", inferSchema=True, header=True)\n",
        "df = df.fillna({'review': ''})                               # Replace nulls with blank string\n",
        "\n",
        "# Add Unique ID\n",
        "df = df.withColumn(\"uid\", monotonically_increasing_id())     # Create Unique ID\n",
        "\n",
        "# Generate YYYY-MM variable\n",
        "df = df.withColumn(\"year_month\", df.date.substr(1,6))\n",
        "\n",
        "# Show rawdata (as DataFrame)\n",
        "df.show(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|uid|year_month|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|  0|    21-Jun|\n",
            "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|  1|    19-Jun|\n",
            "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|  2|    18-Jun|\n",
            "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|  3|    17-Jun|\n",
            "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|  4|    17-Jun|\n",
            "|10006|Delta Air Lines|17-Jun-14|     USA|     9|Business|    5|        YES|Narita - Bangkok ...|  5|    17-Jun|\n",
            "|10007|Delta Air Lines|14-Jun-14|      UK|     0| Economy|    1|         NO|Flight from NY La...|  6|    14-Jun|\n",
            "|10008|Delta Air Lines|14-Jun-14|     USA|     0| Economy|    1|         NO|Originally I had ...|  7|    14-Jun|\n",
            "|10009|Delta Air Lines|13-Jun-14|     USA|     4|Business|    2|         NO|We flew paid busi...|  8|    13-Jun|\n",
            "|10010|Delta Air Lines|13-Jun-14|      UK|     9| Economy|    3|        YES|\"I flew from Heat...|  9|    13-Jun|\n",
            "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gn79OUsC6mP",
        "outputId": "32c6942c-a8f2-4b88-d32f-dd273a3eddc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.createOrReplaceTempView(\"train_df\")\n",
        "sqlDF = spark.sql(\"SELECT * FROM train_df where cabin='Economy'\")\n",
        "sqlDF.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---------------+---------+---------+------+-------+-----+-----------+--------------------+\n",
            "|   id|        airline|     date| location|rating|  cabin|value|recommended|              review|\n",
            "+-----+---------------+---------+---------+------+-------+-----+-----------+--------------------+\n",
            "|10001|Delta Air Lines|21-Jun-14| Thailand|     7|Economy|    4|        YES|Flew Mar 30 NRT t...|\n",
            "|10002|Delta Air Lines|19-Jun-14|      USA|     0|Economy|    2|         NO|Flight 2463 leavi...|\n",
            "|10003|Delta Air Lines|18-Jun-14|      USA|     0|Economy|    1|         NO|Delta Website fro...|\n",
            "|10005|Delta Air Lines|17-Jun-14|  Ecuador|     7|Economy|    3|        YES|\"Round-trip fligh...|\n",
            "|10007|Delta Air Lines|14-Jun-14|       UK|     0|Economy|    1|         NO|Flight from NY La...|\n",
            "|10008|Delta Air Lines|14-Jun-14|      USA|     0|Economy|    1|         NO|Originally I had ...|\n",
            "|10010|Delta Air Lines|13-Jun-14|       UK|     9|Economy|    3|        YES|\"I flew from Heat...|\n",
            "|10011|Delta Air Lines|11-Jun-14|      USA|    10|Economy|    4|        YES|I was a bit stubb...|\n",
            "|10012|Delta Air Lines|10-Jun-14|Australia|    10|Economy|    5|        YES|JFK-LHR. Had a gr...|\n",
            "|10013|Delta Air Lines| 9-Jun-14|      USA|     0|Economy|    1|         NO|My wife and I fly...|\n",
            "|10015|Delta Air Lines| 6-Jun-14|      USA|     0|Economy|    2|         NO|Our flight from F...|\n",
            "|10016|Delta Air Lines| 5-Jun-14|      USA|     0|Economy|    1|         NO|On May 22 after a...|\n",
            "|10017|Delta Air Lines| 3-Jun-14|   Canada|     9|Economy|    4|        YES|Considering how D...|\n",
            "|10018|Delta Air Lines| 2-Jun-14|      USA|     9|Economy|    5|        YES|Travelled MSP-LHR...|\n",
            "|10020|Delta Air Lines|28-May-14|       UK|     9|Economy|    3|        YES|Third long haul f...|\n",
            "|10021|Delta Air Lines|26-May-14|       UK|     2|Economy|    3|         NO|LHR-JFK (Virgin) ...|\n",
            "|10022|Delta Air Lines|24-May-14|      USA|     7|Economy|    3|         NO|Travelled to Lond...|\n",
            "|10023|Delta Air Lines|20-May-14|      USA|     4|Economy|    2|         NO|\"Very disappointe...|\n",
            "|10024|Delta Air Lines|20-May-14|      USA|     2|Economy|    1|         NO|\"Boy are we never...|\n",
            "|10025|Delta Air Lines|19-May-14|   Canada|     9|Economy|    5|        YES|YUL-JFK-NRT-HKG. ...|\n",
            "+-----+---------------+---------+---------+------+-------+-----+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0gEwihnC6mS",
        "outputId": "72975215-486f-482e-f673-fa7fdfa2c97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = df.select('id','review')\n",
        "df.show(10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|   id|              review|\n",
            "+-----+--------------------+\n",
            "|10001|Flew Mar 30 NRT t...|\n",
            "|10002|Flight 2463 leavi...|\n",
            "|10003|Delta Website fro...|\n",
            "|10004|\"I just returned ...|\n",
            "|10005|\"Round-trip fligh...|\n",
            "|10006|Narita - Bangkok ...|\n",
            "|10007|Flight from NY La...|\n",
            "|10008|Originally I had ...|\n",
            "|10009|We flew paid busi...|\n",
            "|10010|\"I flew from Heat...|\n",
            "+-----+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au_sR-FTC6mW",
        "outputId": "f5cbfaf0-edaa-4392-9af9-bbea2497f900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANxqRCPtC6md"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSZsEMHfC6mZ"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJwZvhXqC6mj",
        "outputId": "09b59a26-50c1-4f7d-d95b-1eab1d22d546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "################################################################################################\n",
        "#\n",
        "#   Text Pre-processing (consider using one or all of the following):\n",
        "#       - Remove common words (with stoplist)\n",
        "#       - Handle punctuation\n",
        "#       - lowcase/upcase\n",
        "#       - Stemming\n",
        "#       - Part-of-Speech Tagging (nouns, verbs, adj, etc.)\n",
        "#\n",
        "################################################################################################\n",
        "from pyspark.sql.functions import udf,struct\n",
        "#from pyspark.sql.types import StructType\n",
        "def textprep(record):\n",
        "    text  = record[1]\n",
        "    uid   = record[0]\n",
        "    tokens = text.split()\n",
        "       \n",
        "    # Custom List of Stopwords - Add your own here\n",
        "    tokens = [re.sub('[^a-zA-Z0-9]','',word) for word in tokens]                                       # Remove special characters\n",
        "    tokens = [word.lower() for word in tokens if len(word)>2 and word.lower() not in stop_words_nltk]     # Remove stopwords and words under X length\n",
        "    return tokens\n",
        "\n",
        "udf_textprep = udf(textprep , ArrayType(StringType()))\n",
        "df = df.withColumn(\"words\", udf_textprep(struct([df[x] for x in df.columns])))\n",
        "\n",
        "#tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
        "#wordsData = tokenizer.transform(text)\n",
        "df.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|   id|              review|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|10001|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|\n",
            "|10002|Flight 2463 leavi...|[flight, 2463, le...|\n",
            "|10003|Delta Website fro...|[delta, website, ...|\n",
            "|10004|\"I just returned ...|[returned, roundt...|\n",
            "|10005|\"Round-trip fligh...|[roundtrip, fligh...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI9yaPmyC6mm",
        "outputId": "936e63b1-0771-4363-876b-3ae7764b05c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Term Frequency Vectorization  - Option 1 (Using hashingTF): \n",
        "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "#featurizedData = hashingTF.transform(clean_text)\n",
        "\n",
        "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
        "#cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "cvmodel = cv.fit(df)\n",
        "featurizedData = cvmodel.transform(df)\n",
        "\n",
        "vocab = cvmodel.vocabulary\n",
        "vocab_broadcast = sc.broadcast(vocab)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "df = idfModel.transform(featurizedData)\n",
        "df.show(5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|   id|              review|               words|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|10001|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|(7306,[0,3,13,26,...|(7306,[0,3,13,26,...|\n",
            "|10002|Flight 2463 leavi...|[flight, 2463, le...|(7306,[0,1,6,9,16...|(7306,[0,1,6,9,16...|\n",
            "|10003|Delta Website fro...|[delta, website, ...|(7306,[0,3,4,9,17...|(7306,[0,3,4,9,17...|\n",
            "|10004|\"I just returned ...|[returned, roundt...|(7306,[0,1,2,3,5,...|(7306,[0,1,2,3,5,...|\n",
            "|10005|\"Round-trip fligh...|[roundtrip, fligh...|(7306,[0,4,9,11,1...|(7306,[0,4,9,11,1...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZaGGa0QC6mo",
        "outputId": "cd8d8475-d8df-4be5-9c9e-8c0085078cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Generate 25 Data-Driven Topics:\n",
        "lda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
        "\n",
        "ldamodel = lda.fit(df)\n",
        "\n",
        "#model.isDistributed()\n",
        "#model.vocabSize()\n",
        "\n",
        "ldatopics = ldamodel.describeTopics()\n",
        "#ldatopics.show(25)\n",
        "\n",
        "def map_termID_to_Word(termIndices):\n",
        "    words = []\n",
        "    for termID in termIndices:\n",
        "        words.append(vocab_broadcast.value[termID])\n",
        "    \n",
        "    return words\n",
        "\n",
        "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
        "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
        "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-------------------------------------------------------------------------------------------+\n",
            "|topic|topic_desc                                                                                 |\n",
            "+-----+-------------------------------------------------------------------------------------------+\n",
            "|0    |[today, vegas, las, iad, delta, first, text, received, united, telling]                    |\n",
            "|1    |[nov, 2013, los, angeles, customers, tokyo, far, storage, service, dont]                   |\n",
            "|2    |[sandwich, service, served, united, salad, april, lack, broken, wheelchair, class]         |\n",
            "|3    |[hrs, philadelphia, chicago, year, philly, toronto, status, connection, check, luggage]    |\n",
            "|4    |[economy, international, extra, much, food, manchester, plenty, meals, bkk, crew]          |\n",
            "|5    |[crew, paris, one, asked, water, dallas, passengers, newark, first, milk]                  |\n",
            "|6    |[terrible, march, seats, thing, uncomfortable, lost, entertainment, charge, flights, cana] |\n",
            "|7    |[delay, hour, clt, weather, atlanta, around, sign, hotel, ticket, offer]                   |\n",
            "|8    |[seat, boarding, bags, mother, group, good, san, partner, board, offered]                  |\n",
            "|9    |[line, next, seat, available, got, hotel, left, phx, put, booked]                          |\n",
            "|10   |[would, person, southwest, delayed, issue, going, plane, extremely, tarmac, sat]           |\n",
            "|11   |[class, 1st, row, delta, seat, system, economy, excellent, comfort, american]              |\n",
            "|12   |[orlando, charlotte, cancelled, told, jacksonville, lax, credit, check, traveling, airways]|\n",
            "|13   |[voucher, dtw, dublin, bag, day, louis, soft, baggage, planes, rebook]                     |\n",
            "|14   |[good, crew, great, comfortable, seats, quick, overall, delta, cabin, excellent]           |\n",
            "|15   |[canada, airways, american, good, drink, economy, philadelphia, great, ontime, air]        |\n",
            "|16   |[fuel, tickets, get, delta, plus, give, together, needed, airport, destination]            |\n",
            "|17   |[deltas, phl, delta, business, sydney, united, lax, mexico, syd, changed]                  |\n",
            "|18   |[phoenix, gate, san, diego, united, day, agent, orleans, connection, boarding]             |\n",
            "|19   |[detroit, atl, call, horrible, boarding, got, gate, flights, went, counter]                |\n",
            "|20   |[york, new, way, oakland, wheelchair, fit, uniteds, staff, many, like]                     |\n",
            "|21   |[checkin, son, united, carry, envoy, wife, aircraft, taken, class, continental]            |\n",
            "|22   |[airport, pilot, delayed, cancelled, waiting, hours, maintenance, customer, later, minutes]|\n",
            "|23   |[denver, late, milwaukee, return, hours, connection, washington, asked, plane, without]    |\n",
            "|24   |[denver, seats, delta, kids, poor, seattle, atlanta, airplane, airlines, needs]            |\n",
            "+-----+-------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PLOB_wdC6mr",
        "outputId": "06ceb311-0b14-40f1-abdc-e218a97388a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ldaResults = ldamodel.transform(df)\n",
        "\n",
        "ldaResults.select('words','features','topicDistribution').show(5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|               words|            features|   topicDistribution|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|[flew, mar, nrt, ...|(7306,[0,3,13,26,...|[0.02137071958196...|\n",
            "|[flight, 2463, le...|(7306,[0,1,6,9,16...|[0.01450591091449...|\n",
            "|[delta, website, ...|(7306,[0,3,4,9,17...|[0.01751801012174...|\n",
            "|[returned, roundt...|(7306,[0,1,2,3,5,...|[0.01077578417625...|\n",
            "|[roundtrip, fligh...|(7306,[0,4,9,11,1...|[0.00889724273733...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwK6qgQIB0K",
        "outputId": "aa8c3b76-d88a-4850-e76e-323721e7ca77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ldaResults.columns"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'review', 'words', 'rawFeatures', 'features', 'topicDistribution']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47VmTHYC6mv",
        "outputId": "f1b3b4d8-3445-4559-cc03-98ed7dda7f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def maintop(record):\n",
        "    vectorlist = record.tolist()\n",
        "    m = max(vectorlist)\n",
        "    maintops = [i for i, j in enumerate(vectorlist) if j == m] \n",
        "    return maintops\n",
        "\n",
        "def sorttop(record):\n",
        "    vectorlist = record.tolist()\n",
        "    unsorted = [(i,j) for i,j in enumerate(vectorlist)]\n",
        "    maintops = [i for i,j in sorted(unsorted, key=lambda tup: -tup[1])]\n",
        "    return maintops[:5]\n",
        "\n",
        "def maintop2(record):\n",
        "    return record.tolist()\n",
        "\n",
        "udf_maintop = udf(maintop, ArrayType(DoubleType()))\n",
        "udf_maintop2 = udf(maintop2, ArrayType(DoubleType()))\n",
        "udf_maintop3 = udf(sorttop, ArrayType(IntegerType()))\n",
        "\n",
        "# Extract document weights for Topics 0 and 20\n",
        "enrichedData = ldaResults.withColumn(\"MainTopics\", udf_maintop3(ldaResults.topicDistribution))\n",
        "enrichedData = enrichedData.withColumn(\"MainTopic\", enrichedData.MainTopics[0])\n",
        "\n",
        "enrichedData.select('MainTopic','MainTopics').show(5,False)\n",
        "\n",
        "enrichedData.groupBy('MainTopic').count().sort('count', ascending=False).show()\n",
        "\n",
        "#enrichedData.agg(max(\"Topic_12\")).show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------------------+\n",
            "|MainTopic|MainTopics          |\n",
            "+---------+--------------------+\n",
            "|4        |[4, 15, 24, 14, 17] |\n",
            "|13       |[13, 20, 22, 18, 23]|\n",
            "|17       |[17, 10, 19, 9, 14] |\n",
            "|19       |[19, 14, 4, 8, 2]   |\n",
            "|7        |[7, 2, 12, 13, 9]   |\n",
            "+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+-----+\n",
            "|MainTopic|count|\n",
            "+---------+-----+\n",
            "|       22|   61|\n",
            "|       19|   47|\n",
            "|       11|   45|\n",
            "|        4|   44|\n",
            "|        8|   44|\n",
            "|        3|   43|\n",
            "|       13|   43|\n",
            "|       15|   42|\n",
            "|        2|   42|\n",
            "|       12|   41|\n",
            "|       16|   39|\n",
            "|       17|   39|\n",
            "|        1|   39|\n",
            "|       14|   38|\n",
            "|       23|   38|\n",
            "|       10|   38|\n",
            "|       24|   38|\n",
            "|        6|   37|\n",
            "|        7|   37|\n",
            "|        9|   35|\n",
            "+---------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vTYmNbjC6mz",
        "outputId": "1886cc17-19ad-4cb1-9f64-db900a4e1b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "enrichedData.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
            "|   id|              review|               words|         rawFeatures|            features|   topicDistribution|          MainTopics|MainTopic|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
            "|10001|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|(7306,[0,3,13,26,...|(7306,[0,3,13,26,...|[0.02137071958196...| [4, 15, 24, 14, 17]|        4|\n",
            "|10002|Flight 2463 leavi...|[flight, 2463, le...|(7306,[0,1,6,9,16...|(7306,[0,1,6,9,16...|[0.01450591091449...|[13, 20, 22, 18, 23]|       13|\n",
            "|10003|Delta Website fro...|[delta, website, ...|(7306,[0,3,4,9,17...|(7306,[0,3,4,9,17...|[0.01751801012174...| [17, 10, 19, 9, 14]|       17|\n",
            "|10004|\"I just returned ...|[returned, roundt...|(7306,[0,1,2,3,5,...|(7306,[0,1,2,3,5,...|[0.01077578417625...|   [19, 14, 4, 8, 2]|       19|\n",
            "|10005|\"Round-trip fligh...|[roundtrip, fligh...|(7306,[0,4,9,11,1...|(7306,[0,4,9,11,1...|[0.00889724273733...|   [7, 2, 12, 13, 9]|        7|\n",
            "|10006|Narita - Bangkok ...|[narita, bangkok,...|(7306,[0,2,3,5,10...|(7306,[0,2,3,5,10...|[0.05335624483973...|    [1, 7, 0, 8, 14]|        1|\n",
            "|10007|Flight from NY La...|[flight, guardia,...|(7306,[0,5,6,9,16...|(7306,[0,5,6,9,16...|[0.01776059678025...|[16, 17, 22, 24, 18]|       16|\n",
            "|10008|Originally I had ...|[originally, hour...|(7306,[3,4,9,16,1...|(7306,[3,4,9,16,1...|[0.08164364617131...|  [7, 0, 11, 16, 15]|        7|\n",
            "|10009|We flew paid busi...|[flew, paid, busi...|(7306,[0,1,2,4,8,...|(7306,[0,1,2,4,8,...|[0.00404786733312...|  [17, 4, 14, 11, 1]|       17|\n",
            "|10010|\"I flew from Heat...|[flew, heathrow, ...|(7306,[0,2,9,11,1...|(7306,[0,2,9,11,1...|[0.11783951154128...|    [9, 0, 4, 14, 7]|        9|\n",
            "|10011|I was a bit stubb...|[bit, stubborn, f...|(7306,[0,1,9,10,1...|(7306,[0,1,9,10,1...|[0.02649599834080...| [16, 18, 19, 5, 14]|       16|\n",
            "|10012|JFK-LHR. Had a gr...|[jfklhr, great, t...|(7306,[0,1,2,5,8,...|(7306,[0,1,2,5,8,...|[0.02514545584431...|   [22, 4, 8, 14, 1]|       22|\n",
            "|10013|My wife and I fly...|[wife, fly, frequ...|(7306,[0,5,9,12,1...|(7306,[0,5,9,12,1...|[0.03043002114104...| [24, 3, 23, 22, 16]|       24|\n",
            "|10014|DL 1134 PBI-ATL. ...|[1134, pbiatl, gr...|(7306,[0,1,3,4,9,...|(7306,[0,1,3,4,9,...|[0.01484816872311...|   [6, 5, 7, 14, 16]|        6|\n",
            "|10015|Our flight from F...|[flight, fairbank...|(7306,[0,2,3,9,16...|(7306,[0,2,3,9,16...|[0.02984969572208...|  [6, 21, 0, 11, 18]|        6|\n",
            "|10016|On May 22 after a...|[may, 6hour, dela...|(7306,[0,1,9,12,2...|(7306,[0,1,9,12,2...|[0.01051941641517...| [22, 12, 10, 24, 4]|       22|\n",
            "|10017|Considering how D...|[considering, del...|(7306,[0,2,3,5,9,...|(7306,[0,2,3,5,9,...|[0.02177593403844...|  [7, 15, 16, 14, 6]|        7|\n",
            "|10018|Travelled MSP-LHR...|[travelled, msplh...|(7306,[0,1,3,8,9,...|(7306,[0,1,3,8,9,...|[0.04290913397756...| [17, 14, 11, 0, 23]|       17|\n",
            "|10019|JFK-LAX on a 757-...|[jfklax, 757200, ...|(7306,[0,2,8,10,1...|(7306,[0,2,8,10,1...|[0.01286540263349...|  [1, 11, 14, 4, 15]|        1|\n",
            "|10020|Third long haul f...|[third, long, hau...|(7306,[0,1,3,4,8,...|(7306,[0,1,3,4,8,...|[0.02208883016413...|  [15, 14, 9, 6, 19]|       15|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}