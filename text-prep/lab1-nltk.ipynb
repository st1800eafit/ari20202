{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Edwin Montoya-Múnera - emontoya@eafit.edu.co\n",
    "# Universidad EAFIT \n",
    "# 2020-2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar las librerias necesarias\n",
    "## 1. nltk para 'procesamiento natural del lenguaje'\n",
    "## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n",
    "## 3. re - expresiones regulares\n",
    "## 4. numpy, codecs, etc - otras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorios (path) de entrada y salida:\n",
    "# \n",
    "path_in=\"../datasets/gutenberg-es/\"\n",
    "path_out=\"../out/\"\n",
    "filenametxt='pg2000.txt'\n",
    "filenamecleantxt='pg2000_clean.txt'\n",
    "filenamecsv='pg2000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/emontoya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emontoya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus de nltk para 'tokenizer' y 'stopwords'\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['texto', 'libre', 'que', 'permite', 'crear', 'hiso20091iras', 'epor', '--', '4', 'no', 's', '#', 'e', 'preocupe', 'hola', 'mundo', 'cruel']\n"
     ]
    }
   ],
   "source": [
    "# ejemplo de como nltk tokeniza:\n",
    "texto=\"texto libre que permite crear     hiso20091iras epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "['texto', 'libre', 'que', 'permite', 'crear', 'hiso20091iras', 'epor--4', 'no', 's#e', 'preocupe', 'hola', 'mundo', 'cruel']\n",
      "29\n",
      "['texto', 'libre', 'que', 'permite', 'crear', '', '', '', '', 'hiso20091iras', '', '', '', '', '', '', '', '', '', '', '', 'epor--4', 'no', 's#e', 'preocupe', '\\n', 'hola', 'mundo', 'cruel']\n"
     ]
    }
   ],
   "source": [
    "# note la estrategia de tokenizar con sentencias simples de python, \n",
    "# ¿ cual le parece mejor?\n",
    "# y note la diferencia entre .split() y .split(' ')\n",
    "texto=\"texto libre que permite crear     hiso20091iras            epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
    "tokens = texto.split()\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "tokens = texto.split(' ')\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n",
      "['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos', 'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás', 'esté', 'estéis', 'estén', 'estés', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuéramos', 'fuésemos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá', 'habrán', 'habrás', 'habré', 'habréis', 'habría', 'habríais', 'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais', 'habíamos', 'habían', 'habías', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'más', 'mí', 'mía', 'mías', 'mío', 'míos', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean', 'seas', 'seremos', 'será', 'serán', 'serás', 'seré', 'seréis', 'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seáis', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sí', 'también', 'tanto', 'te', 'tendremos', 'tendrá', 'tendrán', 'tendrás', 'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos', 'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengáis', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais', 'teníamos', 'tenían', 'tenías', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviéramos', 'tuviésemos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 'tú', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos']\n"
     ]
    }
   ],
   "source": [
    "# stopwords con 'stop-words'\n",
    "# otra libreria diferentes de nltk para diccionario de stopwords, cual será mejor?\n",
    "# $ pip install stop-words\n",
    "# $ git clone --recursive git://github.com/Alir3z4/python-stop-words.git\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('spanish')\n",
    "stop_words = get_stop_words('es')\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "{'tendríais', 'ella', 'estuvieras', 'seas', 'les', 'tuvierais', 'esas', 'estuvieseis', 'estadas', 'muchos', 'hayamos', 'eras', 'fuera', 'habremos', 'sentidas', 'tuvieses', 'nuestra', 'míos', 'sí', 'tenga', 'otra', 'estas', 'estoy', 'algo', 'nos', 'estaré', 'fuiste', 'no', 'por', 'serás', 'tuyo', 'nosotros', 'sentidos', 'suyo', 'tendríamos', 'tuviese', 'fue', 'hubiéramos', 'habidos', 'y', 'habíamos', 'estarías', 'has', 'hubo', 'teníais', 'yo', 'otras', 'tuvieran', 'hubieran', 'estuvimos', 'quienes', 'tuviste', 'estarán', 'esos', 'la', 'hubiesen', 'habida', 'tendrá', 'tanto', 'estamos', 'tuviesen', 'suya', 'en', 'somos', 'tienen', 'más', 'tenida', 'el', 'tengan', 'tuvo', 'vuestra', 'tienes', 'e', 'tenido', 'fuéramos', 'estará', 'pero', 'estaba', 'con', 'a', 'tendrías', 'me', 'estuvisteis', 'habríamos', 'nosotras', 'teníamos', 'tendréis', 'estuviesen', 'te', 'estuvieron', 'fuimos', 'durante', 'han', 'todos', 'hubierais', 'ha', 'eran', 'tiene', 'hubiésemos', 'estada', 'una', 'he', 'al', 'tenían', 'estuvieses', 'os', 'estabas', 'tengamos', 'serán', 'habríais', 'soy', 'tu', 'estás', 'contra', 'ese', 'otros', 'nuestro', 'sentid', 'seríamos', 'era', 'hubiera', 'hubiese', 'algunas', 'sea', 'tuvimos', 'eres', 'mucho', 'seáis', 'seréis', 'donde', 'estado', 'habría', 'qué', 'estuvieran', 'habías', 'también', 'son', 'estad', 'ante', 'nuestros', 'tú', 'sentido', 'habrán', 'tus', 'mías', 'estados', 'hubieses', 'vuestras', 'su', 'hay', 'esa', 'tuvieras', 'estuvo', 'estaríamos', 'hemos', 'fuisteis', 'suyos', 'habéis', 'estar', 'haya', 'mía', 'habrían', 'estuviéramos', 'es', 'hayas', 'estaría', 'nuestras', 'muy', 'tendrán', 'serías', 'desde', 'quien', 'tenidos', 'tenías', 'tengo', 'fueras', 'seremos', 'serían', 'estuviésemos', 'estarás', 'hubieseis', 'estuve', 'o', 'nada', 'vuestros', 'algunos', 'seríais', 'otro', 'eso', 'tengáis', 'erais', 'estaban', 'tuve', 'tuvisteis', 'estos', 'tenía', 'sobre', 'este', 'fueseis', 'hubiste', 'ti', 'tuya', 'estaréis', 'hubisteis', 'estarían', 'esta', 'habidas', 'los', 'unos', 'estén', 'tuyos', 'sois', 'tenéis', 'seamos', 'tendría', 'habrás', 'que', 'estéis', 'todo', 'ya', 'habido', 'tuviéramos', 'habrá', 'fueran', 'entre', 'sus', 'será', 'mi', 'había', 'tengas', 'estaremos', 'antes', 'siente', 'fueses', 'estuviese', 'tuviésemos', 'cuando', 'estuviste', 'tendré', 'hasta', 'ni', 'habiendo', 'estés', 'tendremos', 'poco', 'estábamos', 'cual', 'sería', 'habíais', 'teniendo', 'seré', 'habrías', 'sin', 'estemos', 'un', 'como', 'vosotros', 'estuvierais', 'vuestro', 'fui', 'tendrían', 'lo', 'sean', 'mis', 'mío', 'tendrás', 'estuviera', 'tened', 'fuese', 'hubimos', 'fueron', 'tuvieron', 'se', 'estando', 'habré', 'hubieras', 'tuviera', 'ellas', 'le', 'esté', 'tuvieseis', 'hubieron', 'para', 'las', 'esto', 'estabais', 'fuesen', 'sintiendo', 'ellos', 'porque', 'sentida', 'de', 'del', 'fuerais', 'estáis', 'hayan', 'él', 'vosotras', 'está', 'están', 'hayáis', 'estaríais', 'éramos', 'uno', 'tuyas', 'fuésemos', 'suyas', 'habréis', 'hube', 'tenemos', 'habían', 'mí', 'tenidas'}\n"
     ]
    }
   ],
   "source": [
    "# stopwords en nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('spanish'))\n",
    "print(len(stop_words_nltk))\n",
    "print(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n",
    "from nltk.corpus import words as voc_en\n",
    "x = len(voc_en.words())\n",
    "print('tamaño del diccionario en ingles del nltk: ',x)\n",
    "# verifica si una palabra pertenece al diccionario:\n",
    "w = \"house\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")\n",
    "    \n",
    "w = \"pepito\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer un archivo de ejemplo en .txt\n",
    "input_file = open(path_in+filenametxt, \"r\", encoding='iso-8859-1')\n",
    "filedata = input_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 1:\n",
    "# TOKENIZAR con .split(), \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n",
    "# REMOVER stop words con nltk\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = filedata.split()\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "topwords = fdist.most_common(20)\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 2:\n",
    "# TOKENIZAR con nltk, \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos\n",
    "# REMOVER stop words\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = nltk.word_tokenize(filedata)\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming con NLTK\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "# probar cada una de las siguientes opciones: porter y lancaster.\n",
    "#tokens = [porter.stem(w) for w in tokens]\n",
    "tokens = [lancaster.stem(w) for w in tokens]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization con NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# probar cada una de las siguientes opciones: \n",
    "#tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n",
    "tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volver a leer el archivo ejemplo en .txt\n",
    "#input_file = open(path_in+filenametxt, \"r\",encoding='iso-8859-1')\n",
    "input_file = open(path_in+filenametxt, \"r\")\n",
    "output_file_clean = open(path_out+filenamecleantxt, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for line in input_file:\n",
    "    line_clean = \"\"\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "    #tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    #tokens = [porter.stem(w) for w in tokens]\n",
    "    tokens = [lancaster.stem(w) for w in tokens]\n",
    "    \n",
    "    for w in tokens:\n",
    "        line_clean=line_clean+w+\" \"\n",
    "            \n",
    "    if (line_clean!=\"\"):\n",
    "        line_clean=line_clean+\"\\n\"\n",
    "        output_file_clean.write(line_clean)\n",
    "output_file_clean.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_clean = open(path_out+filenamecleantxt, \"r\", encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedata = input_file_clean.read()\n",
    "tokens = filedata.split()\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = fdist.most_common(len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(path_out+filenamecsv, 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow([\"word\", \"frecuency\"])\n",
    "    writer.writerows(word_freq)\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 30 words\n",
    "top_words = word_freq[:20]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(top_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x,y = zip(*top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(df[0],df[1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
